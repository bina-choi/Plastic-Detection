{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 0. 패키지 import"
      ],
      "metadata": {
        "id": "mpRVJoIMruWU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kts8YnjobIhe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.transforms as T\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "from PIL import Image\n",
        "import json\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from torchvision.ops import box_iou\n",
        "import time\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Google Drive 마운트\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4798TVUbK0T",
        "outputId": "969277f3-ea80-48a1-c6ad-37364dccfbac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 이미지 불러오기 + 변환"
      ],
      "metadata": {
        "id": "Yb_ggz7urzcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#이미지 경로와 annotation 읽어오기\n",
        "# cache를 활용해 한 번 읽어온 이미지를 메모리에 저장해두고, 메모리에서 데이터를 그때그때 불러온다 (드라이브에서 불러오는건 비효율적이기 때문)\n",
        "\n",
        "import json\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import os\n",
        "import weakref\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, json_file, transforms=None):\n",
        "        with open(json_file) as f:\n",
        "            self.data = json.load(f)\n",
        "        self.transforms = transforms\n",
        "        self.cache = weakref.WeakValueDictionary()  # Use weak reference cache to avoid memory leaks\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    # Load images\n",
        "    def _load_image(self, img_path):\n",
        "        if img_path in self.cache:\n",
        "            return self.cache[img_path]\n",
        "\n",
        "        try:\n",
        "            img = Image.open(img_path).convert(\"RGB\")\n",
        "            self.cache[img_path] = img\n",
        "            return img\n",
        "        except FileNotFoundError:\n",
        "            print(f\"File not found: {img_path}\")\n",
        "            return None\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        key = list(self.data.keys())[idx]\n",
        "        img_path = self.data[key][\"image\"]\n",
        "        img = self._load_image(img_path)\n",
        "\n",
        "        if img is None:\n",
        "            return None, None\n",
        "\n",
        "        boxes = torch.as_tensor(self.data[key][\"bbox\"], dtype=torch.float32)\n",
        "        labels = torch.as_tensor(self.data[key][\"label\"], dtype=torch.int64)\n",
        "\n",
        "        # Validate and fix bounding boxes\n",
        "        valid_boxes = []\n",
        "        valid_labels = []\n",
        "        for box, label in zip(boxes, labels):\n",
        "            if box[2] > box[0] and box[3] > box[1]:\n",
        "                valid_boxes.append(box)\n",
        "                valid_labels.append(label)\n",
        "\n",
        "        boxes = torch.stack(valid_boxes) if valid_boxes else torch.zeros((0, 4), dtype=torch.float32)\n",
        "        labels = torch.tensor(valid_labels, dtype=torch.int64) if valid_labels else torch.zeros((0,), dtype=torch.int64)\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "\n",
        "        if self.transforms:\n",
        "            img = self.transforms(img)\n",
        "\n",
        "        return img, target\n"
      ],
      "metadata": {
        "id": "VeXzNzBRgr9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터 변환 함수(학습 시 무작위로 좌우반전)\n",
        "def get_transform(train):\n",
        "    transforms = []\n",
        "    transforms.append(T.ToTensor())\n",
        "    if train:\n",
        "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
        "    return T.Compose(transforms)"
      ],
      "metadata": {
        "id": "CaRP6qrbbKwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 데이터 로드"
      ],
      "metadata": {
        "id": "z_dFg6ijvUg5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터 로더 -> 배치로 묶기\n",
        "def collate_fn(batch):\n",
        "    batch = [b for b in batch if b[0] is not None and b[1] is not None]  # Filter out None values\n",
        "    return tuple(zip(*batch)) if batch else ([], [])"
      ],
      "metadata": {
        "id": "DZlXq19ZbKts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "train_dataset = CustomDataset('/content/drive/MyDrive/preprocessed_data_aug/dataset_train.json', get_transform(train=True))\n",
        "valid_dataset = CustomDataset('/content/drive/MyDrive/preprocessed_data_aug/dataset_valid.json', get_transform(train=False))\n",
        "test_dataset = CustomDataset('/content/drive/MyDrive/preprocessed_data_aug/dataset_test.json', get_transform(train=False))\n",
        "\n",
        "#배치 사이즈 16으로 진행(메모리 부족)\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "KZ3a_z1xbKJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# resnet_50 Model\n",
        "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "num_classes = 16  # Assuming 15 classes + background\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "model.roi_heads.box_predictor = fasterrcnn_resnet50_fpn(num_classes=num_classes).roi_heads.box_predictor"
      ],
      "metadata": {
        "id": "5GRg80sQbKGK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0230975-1838-4fbf-e8c6-12934047bb52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n",
            "100%|██████████| 160M/160M [00:00<00:00, 213MB/s]\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 193MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#gpu 연결\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hqeNd4Zdf1i",
        "outputId": "df7638a2-d39a-424f-e94c-7e49eed36c17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FasterRCNN(\n",
              "  (transform): GeneralizedRCNNTransform(\n",
              "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
              "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
              "  )\n",
              "  (backbone): BackboneWithFPN(\n",
              "    (body): IntermediateLayerGetter(\n",
              "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "      (layer1): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
              "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer2): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
              "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer3): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (3): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (4): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (5): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
              "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (layer4): Sequential(\n",
              "        (0): Bottleneck(\n",
              "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "          (downsample): Sequential(\n",
              "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          )\n",
              "        )\n",
              "        (1): Bottleneck(\n",
              "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Bottleneck(\n",
              "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
              "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
              "          (relu): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (fpn): FeaturePyramidNetwork(\n",
              "      (inner_blocks): ModuleList(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (2): Conv2dNormActivation(\n",
              "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (3): Conv2dNormActivation(\n",
              "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (layer_blocks): ModuleList(\n",
              "        (0-3): 4 x Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        )\n",
              "      )\n",
              "      (extra_blocks): LastLevelMaxPool()\n",
              "    )\n",
              "  )\n",
              "  (rpn): RegionProposalNetwork(\n",
              "    (anchor_generator): AnchorGenerator()\n",
              "    (head): RPNHead(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (1): ReLU(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
              "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
              "    )\n",
              "  )\n",
              "  (roi_heads): RoIHeads(\n",
              "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
              "    (box_head): TwoMLPHead(\n",
              "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
              "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "    )\n",
              "    (box_predictor): FastRCNNPredictor(\n",
              "      (cls_score): Linear(in_features=1024, out_features=16, bias=True)\n",
              "      (bbox_pred): Linear(in_features=1024, out_features=64, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. 모델 훈련"
      ],
      "metadata": {
        "id": "etQ7af1avbjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function\n",
        "def train_one_epoch(model, data_loader, optimizer, device, epoch):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, targets in tqdm(data_loader, desc=f\"Epoch {epoch}\"):\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += losses.item()\n",
        "    epoch_loss = running_loss / len(data_loader)\n",
        "    return epoch_loss"
      ],
      "metadata": {
        "id": "aAf9KDNUbKEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#IOU 계산함수 정의\n",
        "def box_iou(box1, box2):\n",
        "    \"\"\"Compute the Intersection Over Union (IOU) of two sets of boxes.\n",
        "    The box order must be (xmin, ymin, xmax, ymax).\n",
        "    \"\"\"\n",
        "    inter = (torch.min(box1[..., None, 2:], box2[..., 2:]) -\n",
        "             torch.max(box1[..., None, :2], box2[..., :2])).clamp(0).prod(2)\n",
        "    area1 = (box1[..., 2:] - box1[..., :2]).prod(1)\n",
        "    area2 = (box2[..., 2:] - box2[..., :2]).prod(1)\n",
        "    union = area1[..., None] + area2 - inter\n",
        "    return inter / union"
      ],
      "metadata": {
        "id": "P1Dt3ryabJ91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#accuracy만 반환하는 함수\n",
        "\n",
        "def evaluate(model, data_loader, device):\n",
        "    model.eval()\n",
        "    total_boxes = 0\n",
        "    correct_boxes = 0\n",
        "    with torch.no_grad():\n",
        "        for images, targets in data_loader:\n",
        "            images = list(image.to(device) for image in images)\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "            outputs = model(images)\n",
        "\n",
        "            for target, output in zip(targets, outputs):\n",
        "                target_boxes = target[\"boxes\"]\n",
        "                target_labels = target[\"labels\"]\n",
        "                pred_boxes = output[\"boxes\"]\n",
        "                pred_labels = output[\"labels\"]\n",
        "\n",
        "                total_boxes += len(target_boxes)\n",
        "\n",
        "                # Create a matching between predicted and target boxes\n",
        "                for i, pred_box in enumerate(pred_boxes):\n",
        "                    pred_label = pred_labels[i]\n",
        "                    matching_indices = (target_labels == pred_label).nonzero(as_tuple=True)[0]\n",
        "\n",
        "                    if len(matching_indices) > 0:\n",
        "                        ious = box_iou(pred_box.unsqueeze(0), target_boxes[matching_indices]).squeeze(0)\n",
        "                        max_iou, max_idx = ious.max(dim=0)\n",
        "                        if max_iou.item() > 0.5:\n",
        "                            correct_boxes += 1\n",
        "                            # Remove the matched box to avoid double counting\n",
        "                            target_boxes = torch.cat((target_boxes[:max_idx], target_boxes[max_idx+1:]))\n",
        "                            target_labels = torch.cat((target_labels[:max_idx], target_labels[max_idx+1:]))\n",
        "\n",
        "    accuracy = correct_boxes / total_boxes if total_boxes > 0 else 0\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "c8wT5uPBI4KP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#mAP 계산 함수 정의\n",
        "\n",
        "def calculate_map(detections, annotations, iou_threshold=0.5):\n",
        "    average_precisions = []\n",
        "    # Assume labels are from 1 to num_classes\n",
        "    num_classes = max([np.max(d['labels']) for d in detections]) if detections else 0\n",
        "\n",
        "    for label in range(1, num_classes + 1):\n",
        "        true_positives = []\n",
        "        scores = []\n",
        "        num_ground_truths = 0\n",
        "\n",
        "        for detection, annotation in zip(detections, annotations):\n",
        "            ground_truths = annotation['boxes'][annotation['labels'] == label]\n",
        "            num_ground_truths += len(ground_truths)\n",
        "            detected = []\n",
        "\n",
        "            for box, score, pred_label in zip(detection['boxes'], detection['scores'], detection['labels']):\n",
        "                if pred_label == label:\n",
        "                    scores.append(score)\n",
        "                    if ground_truths.size > 0:\n",
        "                        ious = box_iou(torch.tensor(box).unsqueeze(0), torch.tensor(ground_truths))\n",
        "                        max_iou = ious.max().item()\n",
        "                        if max_iou > iou_threshold and max_iou not in detected:\n",
        "                            true_positives.append(1)\n",
        "                            detected.append(max_iou)\n",
        "                        else:\n",
        "                            true_positives.append(0)\n",
        "                    else:\n",
        "                        true_positives.append(0)\n",
        "\n",
        "        # Sort by scores\n",
        "        indices = np.argsort(-np.array(scores))\n",
        "        true_positives = np.array(true_positives)[indices]\n",
        "        tp_cumsum = np.cumsum(true_positives)\n",
        "        fp_cumsum = np.cumsum(1 - true_positives)\n",
        "        recalls = tp_cumsum / (num_ground_truths + np.finfo(np.float64).eps)\n",
        "        precisions = tp_cumsum / (tp_cumsum + fp_cumsum + np.finfo(np.float64).eps)\n",
        "\n",
        "        # Calculate average precision\n",
        "        ap = np.trapz(precisions, recalls)\n",
        "        average_precisions.append(ap)\n",
        "\n",
        "    mAP = np.mean(average_precisions) if average_precisions else 0\n",
        "    return mAP"
      ],
      "metadata": {
        "id": "2kva4tEneRpY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#평가지표 출력\n",
        "\n",
        "import torch\n",
        "from torchvision.ops import box_iou\n",
        "import numpy as np\n",
        "\n",
        "def evaluate_index(model, data_loader, device):\n",
        "    model.eval()\n",
        "    total_boxes = 0\n",
        "    true_positives = 0\n",
        "    false_positives = 0\n",
        "    false_negatives = 0\n",
        "    all_detections = []\n",
        "    all_annotations = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets in data_loader:\n",
        "            images = list(image.to(device) for image in images)\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "            outputs = model(images)\n",
        "\n",
        "            for target, output in zip(targets, outputs):\n",
        "                target_boxes = target[\"boxes\"]\n",
        "                target_labels = target[\"labels\"]\n",
        "                pred_boxes = output[\"boxes\"]\n",
        "                pred_labels = output[\"labels\"]\n",
        "\n",
        "                total_boxes += len(target_boxes)\n",
        "\n",
        "                # Create a matching between predicted and target boxes\n",
        "                for i, pred_box in enumerate(pred_boxes):\n",
        "                    pred_label = pred_labels[i]\n",
        "                    matching_indices = (target_labels == pred_label).nonzero(as_tuple=True)[0]\n",
        "\n",
        "                    if len(matching_indices) > 0:\n",
        "                        ious = box_iou(pred_box.unsqueeze(0), target_boxes[matching_indices]).squeeze(0)\n",
        "                        max_iou, max_idx = ious.max(dim=0)\n",
        "                        if max_iou.item() > 0.5:\n",
        "                            true_positives += 1\n",
        "                            # Remove the matched box to avoid double counting\n",
        "                            target_boxes = torch.cat((target_boxes[:max_idx], target_boxes[max_idx+1:]))\n",
        "                            target_labels = torch.cat((target_labels[:max_idx], target_labels[max_idx+1:]))\n",
        "                        else:\n",
        "                            false_positives += 1\n",
        "                    else:\n",
        "                        false_positives += 1\n",
        "\n",
        "                false_negatives += len(target_boxes)\n",
        "\n",
        "                # Collect all annotations and detections for mAP calculation\n",
        "                annotations = {\n",
        "                    'boxes': target['boxes'].cpu().numpy(),\n",
        "                    'labels': target['labels'].cpu().numpy()\n",
        "                }\n",
        "                detections = {\n",
        "                    'boxes': output['boxes'].cpu().numpy(),\n",
        "                    'labels': output['labels'].cpu().numpy(),\n",
        "                    'scores': output['scores'].cpu().numpy()\n",
        "                }\n",
        "                all_annotations.append(annotations)\n",
        "                all_detections.append(detections)\n",
        "\n",
        "    precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n",
        "    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n",
        "    mAP = calculate_map(all_detections, all_annotations)\n",
        "\n",
        "    return precision, recall, mAP"
      ],
      "metadata": {
        "id": "pcCYzaEXSN_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#optimizer 정의\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "optimizer = optim.SGD(params, lr=0.0005, momentum=0.9, weight_decay=0.0005)"
      ],
      "metadata": {
        "id": "ffOlOaw5dodd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "#모델 학습 루프\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_one_epoch(model, train_loader, optimizer, device, epoch)\n",
        "    print(f\"Epoch {epoch}: Train Loss: {train_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BR8JB-adobJ",
        "outputId": "9c9245a8-8484-4dbd-bf81-7fa17d6cf62b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0:  37%|███▋      | 338/907 [1:24:31<2:16:53, 14.43s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#cache 비우기\n",
        "import torch, gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "bCjpWs95x22r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Model 저장 및 학습 결과 출력"
      ],
      "metadata": {
        "id": "ogWCF1q6hnkc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/Colab Notebooks/detection_model_newpre.pth')"
      ],
      "metadata": {
        "id": "G_qTdI1B-Vao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#평가지표 출력\n",
        "precision, recall, mAP = evaluate_index(model, valid_loader, device)\n",
        "print(f\"precision: {precision:.4f}\")\n",
        "print(f\"recall: {recall:.4f}\")\n",
        "print(f\"mAP : {mAP:.4f}\")"
      ],
      "metadata": {
        "id": "a-56wZOQo-Dn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## cf) Loading saved model"
      ],
      "metadata": {
        "id": "13eKQMHuh3cT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model for testing\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/Colab Notebooks/detection_model_newpre.pth'))\n",
        "\n",
        "precision, recall, mAP = evaluate_index(model, valid_loader, device)\n",
        "print(f\"precision: {precision:.4f}\")\n",
        "print(f\"recall: {recall:.4f}\")\n",
        "print(f\"mAP : {mAP:.4f}\")"
      ],
      "metadata": {
        "id": "3WKQJLEdd5dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd004471-9151-4796-d872-cf6b23dbff83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "precision: 0.0521\n",
            "recall: 0.8198\n",
            "mAP : 0.5273\n"
          ]
        }
      ]
    }
  ]
}